root
 |-- crs: struct (nullable = true)
 |    |-- crsProj4: string (nullable = false)
 |-- extent: struct (nullable = true)
 |    |-- xmin: double (nullable = false)
 |    |-- ymin: double (nullable = false)
 |    |-- xmax: double (nullable = false)
 |    |-- ymax: double (nullable = false)
 |-- scl: tile (nullable = true)
 |-- B01: tile (nullable = true)
 |-- B02: tile (nullable = true)
 |-- B03: tile (nullable = true)
 |-- B04: tile (nullable = true)
 |-- B05: tile (nullable = true)
 |-- B06: tile (nullable = true)
 |-- B07: tile (nullable = true)
 |-- B08: tile (nullable = true)
 |-- B09: tile (nullable = true)
 |-- B11: tile (nullable = true)
 |-- B12: tile (nullable = true)

Found  1 distinct CRS.
root
 |-- crs: struct (nullable = true)
 |    |-- crsProj4: string (nullable = false)
 |-- extent: struct (nullable = true)
 |    |-- xmin: double (nullable = false)
 |    |-- ymin: double (nullable = false)
 |    |-- xmax: double (nullable = false)
 |    |-- ymax: double (nullable = false)
 |-- scl: tile (nullable = true)
 |-- B01: tile (nullable = true)
 |-- B02: tile (nullable = true)
 |-- B03: tile (nullable = true)
 |-- B04: tile (nullable = true)
 |-- B05: tile (nullable = true)
 |-- B06: tile (nullable = true)
 |-- B07: tile (nullable = true)
 |-- B08: tile (nullable = true)
 |-- B09: tile (nullable = true)
 |-- B11: tile (nullable = true)
 |-- B12: tile (nullable = true)
 |-- id: long (nullable = true)
 |-- geometry: geometry (nullable = true)
 |-- dims: struct (nullable = true)
 |    |-- cols: integer (nullable = false)
 |    |-- rows: integer (nullable = false)
 |-- label: tile (nullable = true)
 |-- mask: tile (nullable = true)

root
 |-- crs: struct (nullable = true)
 |    |-- crsProj4: string (nullable = false)
 |-- extent: struct (nullable = true)
 |    |-- xmin: double (nullable = false)
 |    |-- ymin: double (nullable = false)
 |    |-- xmax: double (nullable = false)
 |    |-- ymax: double (nullable = false)
 |-- id: long (nullable = true)
 |-- geometry: geometry (nullable = true)
 |-- dims: struct (nullable = true)
 |    |-- cols: integer (nullable = false)
 |    |-- rows: integer (nullable = false)
 |-- column_index: integer (nullable = false)
 |-- row_index: integer (nullable = false)
 |-- scl: double (nullable = false)
 |-- B01: double (nullable = false)
 |-- B02: double (nullable = false)
 |-- B03: double (nullable = false)
 |-- B04: double (nullable = false)
 |-- B05: double (nullable = false)
 |-- B06: double (nullable = false)
 |-- B07: double (nullable = false)
 |-- B08: double (nullable = false)
 |-- B09: double (nullable = false)
 |-- B11: double (nullable = false)
 |-- B12: double (nullable = false)
 |-- label: double (nullable = false)
 |-- mask: double (nullable = false)
 |-- rawPrediction: vector (nullable = true)
 |-- probability: vector (nullable = true)
 |-- prediction: double (nullable = false)


Accuracy: 0.9738621685282274
root
 |-- extent: struct (nullable = true)
 |    |-- xmin: double (nullable = false)
 |    |-- ymin: double (nullable = false)
 |    |-- xmax: double (nullable = false)
 |    |-- ymax: double (nullable = false)
 |-- crs: struct (nullable = true)
 |    |-- crsProj4: string (nullable = false)
 |-- prediction: tile (nullable = true)
 |-- red: tile (nullable = true)
 |-- grn: tile (nullable = true)
 |-- blu: tile (nullable = true)

21/01/19 13:43:14 WARN TileRasterizerAggregate$: You've asked for the construction of a very large image (5580 x 5070). Out of memory error likely.
21/01/19 13:43:14 WARN TileRasterizerAggregate$: You've asked for the construction of a very large image (5580 x 5070). Out of memory error likely.
21/01/19 13:43:14 WARN TileRasterizerAggregate$: You've asked for the construction of a very large image (5580 x 5070). Out of memory error likely.
21/01/19 13:43:14 WARN TileRasterizerAggregate$: You've asked for the construction of a very large image (5580 x 5070). Out of memory error likely.
[Stage 78:>                                                       (0 + 8) / 200]21/01/19 13:43:20 ERROR Executor: Exception in task 3.0 in stage 78.0 (TID 4585)
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:20 ERROR Executor: Exception in task 5.0 in stage 78.0 (TID 4587)
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:20 ERROR Executor: Exception in task 6.0 in stage 78.0 (TID 4588)
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:20 ERROR Executor: Exception in task 7.0 in stage 78.0 (TID 4589)
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:20 ERROR Executor: Exception in task 0.0 in stage 78.0 (TID 4582)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at scala.Array$.ofDim(Array.scala:218)
	at geotrellis.raster.DoubleArrayTile$.fill(DoubleArrayTile.scala:334)
	at geotrellis.raster.DoubleArrayTile$.empty(DoubleArrayTile.scala:302)
	at geotrellis.raster.ArrayTile$.empty(ArrayTile.scala:451)
	at org.locationtech.rasterframes.expressions.aggregates.TileRasterizerAggregate.initialize(TileRasterizerAggregate.scala:63)
	at org.apache.spark.sql.execution.aggregate.ScalaUDAF.initialize(udaf.scala:423)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.initializeBuffer(AggregationIterator.scala:277)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.newBuffer(SortBasedAggregationIterator.scala:68)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:89)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:86)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/01/19 13:43:20 ERROR Executor: Exception in task 2.0 in stage 78.0 (TID 4584)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at scala.Array$.ofDim(Array.scala:218)
	at geotrellis.raster.DoubleArrayTile$.fill(DoubleArrayTile.scala:334)
	at geotrellis.raster.DoubleArrayTile$.empty(DoubleArrayTile.scala:302)
	at geotrellis.raster.ArrayTile$.empty(ArrayTile.scala:451)
	at org.locationtech.rasterframes.expressions.aggregates.TileRasterizerAggregate.initialize(TileRasterizerAggregate.scala:63)
	at org.apache.spark.sql.execution.aggregate.ScalaUDAF.initialize(udaf.scala:423)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.initializeBuffer(AggregationIterator.scala:277)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.newBuffer(SortBasedAggregationIterator.scala:68)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:89)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:86)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/01/19 13:43:20 ERROR Executor: Exception in task 4.0 in stage 78.0 (TID 4586)
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4587,5,main]
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4588,5,main]
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4582,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at scala.Array$.ofDim(Array.scala:218)
	at geotrellis.raster.DoubleArrayTile$.fill(DoubleArrayTile.scala:334)
	at geotrellis.raster.DoubleArrayTile$.empty(DoubleArrayTile.scala:302)
	at geotrellis.raster.ArrayTile$.empty(ArrayTile.scala:451)
	at org.locationtech.rasterframes.expressions.aggregates.TileRasterizerAggregate.initialize(TileRasterizerAggregate.scala:63)
	at org.apache.spark.sql.execution.aggregate.ScalaUDAF.initialize(udaf.scala:423)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.initializeBuffer(AggregationIterator.scala:277)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.newBuffer(SortBasedAggregationIterator.scala:68)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:89)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:86)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4585,5,main]
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4584,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at scala.Array$.ofDim(Array.scala:218)
	at geotrellis.raster.DoubleArrayTile$.fill(DoubleArrayTile.scala:334)
	at geotrellis.raster.DoubleArrayTile$.empty(DoubleArrayTile.scala:302)
	at geotrellis.raster.ArrayTile$.empty(ArrayTile.scala:451)
	at org.locationtech.rasterframes.expressions.aggregates.TileRasterizerAggregate.initialize(TileRasterizerAggregate.scala:63)
	at org.apache.spark.sql.execution.aggregate.ScalaUDAF.initialize(udaf.scala:423)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.initializeBuffer(AggregationIterator.scala:277)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.newBuffer(SortBasedAggregationIterator.scala:68)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:89)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:86)
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4586,5,main]
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4589,5,main]
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR Executor: Exception in task 1.0 in stage 78.0 (TID 4583)
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 4583,5,main]
java.lang.OutOfMemoryError: Java heap space
21/01/19 13:43:21 WARN TaskSetManager: Lost task 4.0 in stage 78.0 (TID 4586, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space

21/01/19 13:43:21 ERROR TaskSetManager: Task 4 in stage 78.0 failed 1 times; aborting job
Traceback (most recent call last):
  File "/home/jenniferwu/Documents/Python_projects/GitLab/ML-algorithms-uvicorn-gunicorn-fastapi/app/algorithms/supervised_machine_learning.py", line 225, in <module>
    retiled.select('prediction', 'red', 'grn', 'blu', 'extent', 'crs').write.geotiff(outfile, crs='EPSG:4326', raster_dimensions=(5580, 5070))
  File "/root/anaconda3/envs/RasterFrames/lib/python3.7/site-packages/pyrasterframes/__init__.py", line 277, in _geotiff_writer
    return _aliased_writer(df_writer, "geotiff", path, **options)
  File "/root/anaconda3/envs/RasterFrames/lib/python3.7/site-packages/pyrasterframes/__init__.py", line 107, in _aliased_writer
    return df_writer.format(format_key).save(path, **options)
  File "/opt/spark/spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/readwriter.py", line 739, in save
    self._jwrite.save(path)
  File "/opt/spark/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/spark/spark-2.4.6-bin-hadoop2.7/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/opt/spark/spark-2.4.6-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o476.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 78.0 failed 1 times, most recent failure: Lost task 4.0 in stage 78.0 (TID 4586, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2557)
	at org.apache.spark.sql.Dataset.first(Dataset.scala:2564)
	at org.locationtech.rasterframes.expressions.aggregates.TileRasterizerAggregate$.collect(TileRasterizerAggregate.scala:166)
	at org.locationtech.rasterframes.datasource.geotiff.GeoTiffDataSource.createRelation(GeoTiffDataSource.scala:85)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:677)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:286)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:272)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space


Process finished with exit code 1
